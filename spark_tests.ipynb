{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/21 19:26:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/21 19:26:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "spark = SparkSession(sc).builder.appName(\"Test Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_command = \"\"\"\n",
    "    SELECT\n",
    "        scraped_at,\n",
    "        company_name,\n",
    "        job_title,\n",
    "        location,\n",
    "        job_page_url\n",
    "    FROM jobs\n",
    "    ORDER BY job_id DESC\n",
    "    ;\"\"\"\n",
    "\n",
    "api_url = \"http://192.168.0.29:5000/execute_sql\"\n",
    "response = requests.post(api_url, json={\"sql_command\": sql_command})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `TimestampType()` can not accept object `2024-06-21 17:34:12.853887` in type `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m    StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraped_at\u001b[39m\u001b[38;5;124m\"\u001b[39m, TimestampType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m    StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m    StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_page_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m ])\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             },\n\u001b[1;32m   2158\u001b[0m         )\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/types.py:2181\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2180\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2181\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_tut-P5kSPYmM/lib/python3.11/site-packages/pyspark/sql/types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2012\u001b[0m             },\n\u001b[1;32m   2013\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `TimestampType()` can not accept object `2024-06-21 17:34:12.853887` in type `str`."
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "   StructField(\"scraped_at\", TimestampType(), False),\n",
    "   StructField(\"company_name\", StringType(), False),\n",
    "   StructField(\"job_title\", StringType(), False),\n",
    "   StructField(\"location\", StringType(), False),\n",
    "   StructField(\"job_page_url\", StringType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "columns = [\n",
    "    \"scraped_at_str\",\n",
    "    \"company_name\",\n",
    "    \"job_title\",\n",
    "    \"location\",\n",
    "    \"job_page_url\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert string to timestamp\n",
    "df = df.withColumn(\"scraped_at\", to_timestamp(col(\"scraped_at_str\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|          scraped_at|  company_name|           job_title|            location|        job_page_url|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|2024-06-21 17:34:...|         Glean|Platform Security...|       Palo Alto, CA|https://boards.gr...|\n",
      "|2024-06-21 17:33:...|Fortune Brands|People Services S...|25300 Al Moen Dri...|https://jobs.smar...|\n",
      "|2024-06-21 17:31:...|      Experian|Data Acquisition ...|955 American Lane...|https://jobs.smar...|\n",
      "|2024-06-21 17:31:...|      Experian|Procurement Contr...|475 Anton Blvd, C...|https://jobs.smar...|\n",
      "|2024-06-21 17:31:...|      Experian|Revenue Cycle Sol...|., ., ., United S...|https://jobs.smar...|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_columns = [\n",
    "    \"scraped_at\",\n",
    "    \"company_name\",\n",
    "    \"job_title\",\n",
    "    \"location\",\n",
    "    \"job_page_url\"\n",
    "]\n",
    "df.select(new_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      scraped_at_str|company_name|           job_title|            location|        job_page_url|          scraped_at|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2024-06-21 17:31:...|    Experian|Data Acquisition ...|955 American Lane...|https://jobs.smar...|2024-06-21 17:31:...|\n",
      "|2024-06-21 17:31:...|    Experian|Procurement Contr...|475 Anton Blvd, C...|https://jobs.smar...|2024-06-21 17:31:...|\n",
      "|2024-06-21 17:31:...|    Experian|Revenue Cycle Sol...|., ., ., United S...|https://jobs.smar...|2024-06-21 17:31:...|\n",
      "|2024-06-21 17:31:...|    Experian|Revenue Cycle Sol...|., ., ., United S...|https://jobs.smar...|2024-06-21 17:31:...|\n",
      "|2024-06-21 15:35:...|    Experian|Analista de Desen...|Avenida das Naçõe...|https://jobs.smar...|2024-06-21 15:35:...|\n",
      "|2024-06-21 15:35:...|    Experian|Human Resources G...|475 Anton Blvd, C...|https://jobs.smar...|2024-06-21 15:35:...|\n",
      "|2024-06-21 15:35:...|    Experian|Service Operation...|Bogotá- Carrera 7...|https://jobs.smar...|2024-06-21 15:35:...|\n",
      "|2024-06-21 14:11:...|    Experian|Finance Superviso...|Centro Corporativ...|https://jobs.smar...|2024-06-21 14:11:...|\n",
      "|2024-06-21 14:11:...|    Experian|Data Amendments A...|Centro Corporativ...|https://jobs.smar...|2024-06-21 14:11:...|\n",
      "|2024-06-21 14:11:...|    Experian|PV | Analista de ...|Avenida das Naçõe...|https://jobs.smar...|2024-06-21 14:11:...|\n",
      "|2024-06-21 14:11:...|    Experian|PV | Analista de ...|Av. das Nações Un...|https://jobs.smar...|2024-06-21 14:11:...|\n",
      "|2024-06-21 12:30:...|    Experian|  Payroll Supervisor|Centro Corporativ...|https://jobs.smar...|2024-06-21 12:30:...|\n",
      "|2024-06-21 12:30:...|    Experian|Information Secur...|Mere Way, Rudding...|https://jobs.smar...|2024-06-21 12:30:...|\n",
      "|2024-06-21 12:30:...|    Experian|Digital Marketing...|The Sir John Peac...|https://jobs.smar...|2024-06-21 12:30:...|\n",
      "|2024-06-21 11:01:...|    Experian|Product Engineer ...|BLOCK-B, Cyber Pe...|https://jobs.smar...|2024-06-21 11:01:...|\n",
      "|2024-06-21 11:01:...|    Experian|ANALISTA DE FINANZAS|Nueva Costanera 4...|https://jobs.smar...|2024-06-21 11:01:...|\n",
      "|2024-06-21 11:01:...|    Experian|Supervisora de Ca...|Avenida das Naçõe...|https://jobs.smar...|2024-06-21 11:01:...|\n",
      "|2024-06-21 11:01:...|    Experian|Business Developm...|Piazza Velasca, 2...|https://jobs.smar...|2024-06-21 11:01:...|\n",
      "|2024-06-21 09:39:...|    Experian|Appeals Analyst p...|Centro Corporativ...|https://jobs.smar...|2024-06-21 09:39:...|\n",
      "|2024-06-21 09:39:...|    Experian|Senior Financial ...|Centro Corporativ...|https://jobs.smar...|2024-06-21 09:39:...|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.where(df[\"company_name\"] == 'Experian')\n",
    "\n",
    "# Print the resulting data frame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 19:34:40 WARN TaskSetManager: Stage 5 contains a task of very large size (1236 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 19:36:56 WARN TaskSetManager: Stage 14 contains a task of very large size (1236 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|       company_name|count|\n",
      "+-------------------+-----+\n",
      "|        Bosch Group| 5762|\n",
      "|    Publicis Groupe| 4438|\n",
      "|                   | 1170|\n",
      "|       Sopra Steria| 1153|\n",
      "|         ServiceNow| 1046|\n",
      "|               Visa|  964|\n",
      "|                ITW|  591|\n",
      "| Palo Alto Networks|  522|\n",
      "|         Epic Games|  520|\n",
      "|   Sonic Automotive|  514|\n",
      "|           Experian|  508|\n",
      "|                OKX|  488|\n",
      "|    Western Digital|  487|\n",
      "|       NBCUniversal|  476|\n",
      "|WNS Global Services|  465|\n",
      "|              Capco|  429|\n",
      "|               Hibu|  409|\n",
      "|          JD Sports|  384|\n",
      "|              Block|  353|\n",
      "|            Celonis|  330|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(\"company_name\").count().orderBy(\"count\", ascending=False)\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 19:37:28 WARN TaskSetManager: Stage 17 contains a task of very large size (1236 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           job_title|count|\n",
      "+--------------------+-----+\n",
      "|                    | 1170|\n",
      "|Outside Sales Rep...|  401|\n",
      "|     Sales Assistant|  144|\n",
      "|   Account Executive|  122|\n",
      "|Senior Customer R...|  116|\n",
      "|Educational Sales...|  105|\n",
      "|Senior Software E...|   68|\n",
      "|   Software Engineer|   54|\n",
      "|        Art Director|   49|\n",
      "|     Project Manager|   49|\n",
      "|  Dynamic PC Support|   48|\n",
      "|Staff Software En...|   46|\n",
      "|Customer Service ...|   45|\n",
      "|Strategic Develop...|   44|\n",
      "|     Account Manager|   44|\n",
      "|  Service Technician|   43|\n",
      "|Sales Development...|   37|\n",
      "|          Copywriter|   36|\n",
      "|Business Developm...|   35|\n",
      "|Outside Sales Acc...|   34|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(\"job_title\").count().orderBy(\"count\", ascending=False)\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop the Spark session if no longer needed\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_tut",
   "language": "python",
   "name": "spark_tut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
